Index: include/asm-i386/page.h
===================================================================
--- include/asm-i386/page.h	(revision 587)
+++ include/asm-i386/page.h	(working copy)
@@ -189,6 +189,7 @@
 #define pfn_valid(pfn)		((pfn) < max_mapnr)
 #endif /* CONFIG_FLATMEM */
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define phys_to_page(x)		pfn_to_page((unsigned long)(x) >> PAGE_SHIFT)
 
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 
Index: include/asm-x86_64/page.h
===================================================================
--- include/asm-x86_64/page.h	(revision 587)
+++ include/asm-x86_64/page.h	(working copy)
@@ -125,6 +125,7 @@
 #endif
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define phys_to_page(x)		pfn_to_page((unsigned long)(x) >> PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #define pfn_to_kaddr(pfn)      __va((pfn) << PAGE_SHIFT)
 
Index: include/linux/kernel.h
===================================================================
--- include/linux/kernel.h	(revision 587)
+++ include/linux/kernel.h	(working copy)
@@ -141,6 +141,8 @@
 
 extern int get_option(char **str, int *pint);
 extern char *get_options(const char *str, int nints, int *ints);
+extern int get_longoption (char **str, long *plong);
+extern char *get_longoptions(const char *str, int nlongs, long *longs);
 extern unsigned long long memparse(char *ptr, char **retptr);
 
 extern int core_kernel_text(unsigned long addr);
Index: include/linux/page-flags.h
===================================================================
--- include/linux/page-flags.h	(revision 587)
+++ include/linux/page-flags.h	(working copy)
@@ -90,6 +90,8 @@
 #define PG_reclaim		17	/* To be reclaimed asap */
 #define PG_buddy		19	/* Page is free, on buddy lists */
 
+#define PG_badram		20	/* BadRam page */
+
 /* PG_owner_priv_1 users should have descriptive aliases */
 #define PG_checked		PG_owner_priv_1 /* Used by some filesystems */
 
@@ -104,6 +106,14 @@
 #define PG_uncached		31	/* Page has been mapped as uncached */
 #endif
 
+#ifdef CONFIG_BADRAM
+#define PageBad(page)		test_bit(PG_badram, &(page)->flags)
+#define PageSetBad(page)	set_bit(PG_badram, &(page)->flags)
+#define PageTestandSetBad(page)	test_and_set_bit(PG_badram, &(page)->flags)
+#else
+#define PageBad(page)		0
+#endif
+
 /*
  * Manipulation of page state flags
  */
Index: CREDITS
===================================================================
--- CREDITS	(revision 587)
+++ CREDITS	(working copy)
@@ -2817,6 +2817,15 @@
 S: Malvern, Pennsylvania 19355
 S: USA
 
+N: Rick van Rein
+E: rick@vanrein.org
+W: http://rick.vanrein.org/
+D: Memory, the BadRAM subsystem dealing with statically challanged RAM modules.
+S: Haarlebrink 5
+S: 7544 WP  Enschede
+S: The Netherlands
+P: 1024D/89754606  CD46 B5F2 E876 A5EE 9A85  1735 1411 A9C2 8975 4606
+
 N: Stefan Reinauer
 E: stepan@linux.de
 W: http://www.freiburg.linux.de/~stepan/
Index: lib/cmdline.c
===================================================================
--- lib/cmdline.c	(revision 587)
+++ lib/cmdline.c	(working copy)
@@ -114,6 +114,69 @@
 }
 
 /**
+ *	get_longoption - Parse long from an option string
+ *	@str: option string
+ *	@plong: (output) long value parsed from @str
+ *
+ *	Read a long from an option string; if available accept a subsequent
+ *	comma as well.
+ *
+ *	Return values:
+ *	0 : no long in string
+ *	1 : long found, no subsequent comma
+ *	2 : long found including a subsequent comma
+ */
+
+int get_longoption (char **str, long *plong)
+{
+	char *cur = *str;
+
+	if (!cur || !(*cur))
+		return 0;
+	*plong = simple_strtol (cur, str, 0);
+	if (cur == *str)
+		return 0;
+	if (**str == ',') {
+		(*str)++;
+		return 2;
+	}
+
+	return 1;
+}
+
+/**
+ *	get_longoptions - Parse a string into a list of longs
+ *	@str: String to be parsed
+ *	@nlongs: size of long array
+ *	@longs: long array
+ *
+ *	This function parses a string containing a comma-separated
+ *	list of longs.  The parse halts when the array is
+ *	full, or when no more numbers can be retrieved from the
+ *	string.
+ *
+ *	Return value is the character in the string which caused
+ *	the parse to end (typically a null terminator, if @str is
+ *	completely parseable).
+ */
+ 
+char *get_longoptions(const char *str, int nlongs, long *longs)
+{
+	int res, i = 1;
+
+	while (i < nlongs) {
+		res = get_longoption ((char **)&str, longs + i);
+		if (res == 0)
+			break;
+		i++;
+		if (res == 1)
+			break;
+	}
+	longs[0] = i - 1;
+	return (char *)str;
+}
+
+/**
  *	memparse - parse a string with mem suffixes into a number
  *	@ptr: Where parse begins
  *	@retptr: (output) Pointer to next char after parse completes
@@ -151,3 +214,5 @@
 EXPORT_SYMBOL(memparse);
 EXPORT_SYMBOL(get_option);
 EXPORT_SYMBOL(get_options);
+EXPORT_SYMBOL(get_longoption);
+EXPORT_SYMBOL(get_longoptions);
Index: mm/bootmem.c
===================================================================
--- mm/bootmem.c	(revision 587)
+++ mm/bootmem.c	(working copy)
@@ -317,10 +317,12 @@
 	pfn = PFN_DOWN(bdata->node_boot_start);
 	idx = bdata->node_low_pfn - pfn;
 	map = bdata->node_bootmem_map;
+#ifndef CONFIG_BADRAM /* no idea if this is really needed */
 	/* Check physaddr is O(LOG2(BITS_PER_LONG)) page aligned */
 	if (bdata->node_boot_start == 0 ||
 	    ffs(bdata->node_boot_start) - PAGE_SHIFT > ffs(BITS_PER_LONG))
 		gofast = 1;
+#endif
 	for (i = 0; i < idx; ) {
 		unsigned long v = ~map[i / BITS_PER_LONG];
 
Index: mm/page_alloc.c
===================================================================
--- mm/page_alloc.c	(revision 587)
+++ mm/page_alloc.c	(working copy)
@@ -10,6 +10,7 @@
  *  Reshaped it to be a zoned allocator, Ingo Molnar, Red Hat, 1999
  *  Discontiguous memory support, Kanoj Sarcar, SGI, Nov 1999
  *  Zone balancing, Kanoj Sarcar, SGI, Jan 2000
+ *  BadRAM handling, Rick van Rein, Feb 2001
  *  Per cpu hot/cold page lists, bulk allocation, Martin J. Bligh, Sept 2002
  *          (lots of bits borrowed from Ingo Molnar & Andrew Morton)
  */
@@ -74,13 +75,13 @@
  */
 int sysctl_lowmem_reserve_ratio[MAX_NR_ZONES-1] = {
 #ifdef CONFIG_ZONE_DMA
-	 256,
+	256,
 #endif
 #ifdef CONFIG_ZONE_DMA32
-	 256,
+	256,
 #endif
 #ifdef CONFIG_HIGHMEM
-	 32
+	32
 #endif
 };
 
@@ -88,14 +89,14 @@
 
 static char * const zone_names[MAX_NR_ZONES] = {
 #ifdef CONFIG_ZONE_DMA
-	 "DMA",
+	"DMA",
 #endif
 #ifdef CONFIG_ZONE_DMA32
-	 "DMA32",
+	"DMA32",
 #endif
-	 "Normal",
+	"Normal",
 #ifdef CONFIG_HIGHMEM
-	 "HighMem"
+	"HighMem"
 #endif
 };
 
@@ -106,33 +107,33 @@
 static unsigned long __meminitdata dma_reserve;
 
 #ifdef CONFIG_ARCH_POPULATES_NODE_MAP
-  /*
-   * MAX_ACTIVE_REGIONS determines the maxmimum number of distinct
-   * ranges of memory (RAM) that may be registered with add_active_range().
-   * Ranges passed to add_active_range() will be merged if possible
-   * so the number of times add_active_range() can be called is
-   * related to the number of nodes and the number of holes
-   */
-  #ifdef CONFIG_MAX_ACTIVE_REGIONS
-    /* Allow an architecture to set MAX_ACTIVE_REGIONS to save memory */
-    #define MAX_ACTIVE_REGIONS CONFIG_MAX_ACTIVE_REGIONS
-  #else
-    #if MAX_NUMNODES >= 32
-      /* If there can be many nodes, allow up to 50 holes per node */
-      #define MAX_ACTIVE_REGIONS (MAX_NUMNODES*50)
-    #else
-      /* By default, allow up to 256 distinct regions */
-      #define MAX_ACTIVE_REGIONS 256
-    #endif
-  #endif
+/*
+ * MAX_ACTIVE_REGIONS determines the maxmimum number of distinct
+ * ranges of memory (RAM) that may be registered with add_active_range().
+ * Ranges passed to add_active_range() will be merged if possible
+ * so the number of times add_active_range() can be called is
+ * related to the number of nodes and the number of holes
+ */
+#ifdef CONFIG_MAX_ACTIVE_REGIONS
+/* Allow an architecture to set MAX_ACTIVE_REGIONS to save memory */
+#define MAX_ACTIVE_REGIONS CONFIG_MAX_ACTIVE_REGIONS
+#else
+#if MAX_NUMNODES >= 32
+/* If there can be many nodes, allow up to 50 holes per node */
+#define MAX_ACTIVE_REGIONS (MAX_NUMNODES*50)
+#else
+/* By default, allow up to 256 distinct regions */
+#define MAX_ACTIVE_REGIONS 256
+#endif
+#endif
 
-  struct node_active_region __meminitdata early_node_map[MAX_ACTIVE_REGIONS];
-  int __meminitdata nr_nodemap_entries;
-  unsigned long __meminitdata arch_zone_lowest_possible_pfn[MAX_NR_ZONES];
-  unsigned long __meminitdata arch_zone_highest_possible_pfn[MAX_NR_ZONES];
+struct node_active_region __meminitdata early_node_map[MAX_ACTIVE_REGIONS];
+int __meminitdata nr_nodemap_entries;
+unsigned long __meminitdata arch_zone_lowest_possible_pfn[MAX_NR_ZONES];
+unsigned long __meminitdata arch_zone_highest_possible_pfn[MAX_NR_ZONES];
 #ifdef CONFIG_MEMORY_HOTPLUG_RESERVE
-  unsigned long __initdata node_boundary_start_pfn[MAX_NUMNODES];
-  unsigned long __initdata node_boundary_end_pfn[MAX_NUMNODES];
+unsigned long __initdata node_boundary_start_pfn[MAX_NUMNODES];
+unsigned long __initdata node_boundary_end_pfn[MAX_NUMNODES];
 #endif /* CONFIG_MEMORY_HOTPLUG_RESERVE */
 #endif /* CONFIG_ARCH_POPULATES_NODE_MAP */
 
@@ -190,12 +191,12 @@
 static void bad_page(struct page *page)
 {
 	printk(KERN_EMERG "Bad page state in process '%s'\n"
-		KERN_EMERG "page:%p flags:0x%0*lx mapping:%p mapcount:%d count:%d\n"
-		KERN_EMERG "Trying to fix it up, but a reboot is needed\n"
-		KERN_EMERG "Backtrace:\n",
-		current->comm, page, (int)(2*sizeof(unsigned long)),
-		(unsigned long)page->flags, page->mapping,
-		page_mapcount(page), page_count(page));
+			KERN_EMERG "page:%p flags:0x%0*lx mapping:%p mapcount:%d count:%d\n"
+			KERN_EMERG "Trying to fix it up, but a reboot is needed\n"
+			KERN_EMERG "Backtrace:\n",
+			current->comm, page, (int)(2*sizeof(unsigned long)),
+			(unsigned long)page->flags, page->mapping,
+			page_mapcount(page), page_count(page));
 	dump_stack();
 	page->flags &= ~(1 << PG_lru	|
 			1 << PG_private |
@@ -258,13 +259,13 @@
 		bad_page(page);
 
 	if (unlikely(!PageHead(page)))
-			bad_page(page);
+		bad_page(page);
 	__ClearPageHead(page);
 	for (i = 1; i < nr_pages; i++) {
 		struct page *p = page + i;
 
 		if (unlikely(!PageTail(p) |
-				(p->first_page != page)))
+					(p->first_page != page)))
 			bad_page(page);
 		__ClearPageTail(p);
 	}
@@ -323,7 +324,7 @@
  *
  * Assumption: *_mem_map is contiguous at least up to MAX_ORDER
  */
-static inline struct page *
+	static inline struct page *
 __page_find_buddy(struct page *page, unsigned long page_idx, unsigned int order)
 {
 	unsigned long buddy_idx = page_idx ^ (1 << order);
@@ -331,7 +332,7 @@
 	return page + (buddy_idx - page_idx);
 }
 
-static inline unsigned long
+	static inline unsigned long
 __find_combined_index(unsigned long page_idx, unsigned int order)
 {
 	return (page_idx & ~(1 << order));
@@ -351,7 +352,7 @@
  * For recording page's order, we use page_private(page).
  */
 static inline int page_is_buddy(struct page *page, struct page *buddy,
-								int order)
+		int order)
 {
 	if (!pfn_valid_within(page_to_pfn(buddy)))
 		return 0;
@@ -431,18 +432,18 @@
 static inline int free_pages_check(struct page *page)
 {
 	if (unlikely(page_mapcount(page) |
-		(page->mapping != NULL)  |
-		(page_count(page) != 0)  |
-		(page->flags & (
-			1 << PG_lru	|
-			1 << PG_private |
-			1 << PG_locked	|
-			1 << PG_active	|
-			1 << PG_slab	|
-			1 << PG_swapcache |
-			1 << PG_writeback |
-			1 << PG_reserved |
-			1 << PG_buddy ))))
+				(page->mapping != NULL)  |
+				(page_count(page) != 0)  |
+				(page->flags & (
+						1 << PG_lru	|
+						1 << PG_private |
+						1 << PG_locked	|
+						1 << PG_active	|
+						1 << PG_slab	|
+						1 << PG_swapcache |
+						1 << PG_writeback |
+						1 << PG_reserved |
+						1 << PG_buddy ))))
 		bad_page(page);
 	/*
 	 * PageReclaim == PageTail. It is only an error
@@ -472,7 +473,7 @@
  * pinned" detection logic.
  */
 static void free_pages_bulk(struct zone *zone, int count,
-					struct list_head *list, int order)
+		struct list_head *list, int order)
 {
 	spin_lock(&zone->lock);
 	zone->all_unreclaimable = 0;
@@ -527,9 +528,11 @@
 {
 	if (order == 0) {
 		__ClearPageReserved(page);
-		set_page_count(page, 0);
-		set_page_refcounted(page);
-		__free_page(page);
+		if (!PageBad(page)) {
+			set_page_count(page, 0);
+			set_page_refcounted(page);
+			__free_page(page);
+		}
 	} else {
 		int loop;
 
@@ -564,7 +567,7 @@
  * -- wli
  */
 static inline void expand(struct zone *zone, struct page *page,
- 	int low, int high, struct free_area *area)
+		int low, int high, struct free_area *area)
 {
 	unsigned long size = 1 << high;
 
@@ -585,20 +588,20 @@
 static int prep_new_page(struct page *page, int order, gfp_t gfp_flags)
 {
 	if (unlikely(page_mapcount(page) |
-		(page->mapping != NULL)  |
-		(page_count(page) != 0)  |
-		(page->flags & (
-			1 << PG_lru	|
-			1 << PG_private	|
-			1 << PG_locked	|
-			1 << PG_active	|
-			1 << PG_dirty	|
-			1 << PG_reclaim	|
-			1 << PG_slab    |
-			1 << PG_swapcache |
-			1 << PG_writeback |
-			1 << PG_reserved |
-			1 << PG_buddy ))))
+				(page->mapping != NULL)  |
+				(page_count(page) != 0)  |
+				(page->flags & (
+						1 << PG_lru	|
+						1 << PG_private	|
+						1 << PG_locked	|
+						1 << PG_active	|
+						1 << PG_dirty	|
+						1 << PG_reclaim	|
+						1 << PG_slab    |
+						1 << PG_swapcache |
+						1 << PG_writeback |
+						1 << PG_reserved |
+						1 << PG_buddy ))))
 		bad_page(page);
 
 	/*
@@ -659,10 +662,10 @@
  * Returns the number of new pages which were placed at *list.
  */
 static int rmqueue_bulk(struct zone *zone, unsigned int order, 
-			unsigned long count, struct list_head *list)
+		unsigned long count, struct list_head *list)
 {
 	int i;
-	
+
 	spin_lock(&zone->lock);
 	for (i = 0; i < count; ++i) {
 		struct page *page = __rmqueue(zone, order);
@@ -808,7 +811,7 @@
 {
 	free_hot_cold_page(page, 0);
 }
-	
+
 void fastcall free_cold_page(struct page *page)
 {
 	free_hot_cold_page(page, 1);
@@ -838,7 +841,7 @@
  * or two.
  */
 static struct page *buffered_rmqueue(struct zonelist *zonelist,
-			struct zone *zone, int order, gfp_t gfp_flags)
+		struct zone *zone, int order, gfp_t gfp_flags)
 {
 	unsigned long flags;
 	struct page *page;
@@ -854,7 +857,7 @@
 		local_irq_save(flags);
 		if (!pcp->count) {
 			pcp->count = rmqueue_bulk(zone, 0,
-						pcp->batch, &pcp->list);
+					pcp->batch, &pcp->list);
 			if (unlikely(!pcp->count))
 				goto failed;
 		}
@@ -941,18 +944,18 @@
 	int err;
 
 	err = init_fault_attr_dentries(&fail_page_alloc.attr,
-				       "fail_page_alloc");
+			"fail_page_alloc");
 	if (err)
 		return err;
 	dir = fail_page_alloc.attr.dentries.dir;
 
 	fail_page_alloc.ignore_gfp_wait_file =
 		debugfs_create_bool("ignore-gfp-wait", mode, dir,
-				      &fail_page_alloc.ignore_gfp_wait);
+				&fail_page_alloc.ignore_gfp_wait);
 
 	fail_page_alloc.ignore_gfp_highmem_file =
 		debugfs_create_bool("ignore-gfp-highmem", mode, dir,
-				      &fail_page_alloc.ignore_gfp_highmem);
+				&fail_page_alloc.ignore_gfp_highmem);
 
 	if (!fail_page_alloc.ignore_gfp_wait_file ||
 			!fail_page_alloc.ignore_gfp_highmem_file) {
@@ -983,7 +986,7 @@
  * of the allocation.
  */
 int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
-		      int classzone_idx, int alloc_flags)
+		int classzone_idx, int alloc_flags)
 {
 	/* free_pages my go negative - that's OK */
 	long min = mark;
@@ -1048,8 +1051,8 @@
 	}
 
 	allowednodes = !in_interrupt() && (alloc_flags & ALLOC_CPUSET) ?
-					&cpuset_current_mems_allowed :
-					&node_online_map;
+		&cpuset_current_mems_allowed :
+		&node_online_map;
 	return allowednodes;
 }
 
@@ -1076,7 +1079,7 @@
  * unturned looking for a free page.
  */
 static int zlc_zone_worth_trying(struct zonelist *zonelist, struct zone **z,
-						nodemask_t *allowednodes)
+		nodemask_t *allowednodes)
 {
 	struct zonelist_cache *zlc;	/* cached zonelist speedup info */
 	int i;				/* index of *z in zonelist zones */
@@ -1120,7 +1123,7 @@
 }
 
 static int zlc_zone_worth_trying(struct zonelist *zonelist, struct zone **z,
-				nodemask_t *allowednodes)
+		nodemask_t *allowednodes)
 {
 	return 1;
 }
@@ -1134,7 +1137,7 @@
  * get_page_from_freelist goes through the zonelist trying to allocate
  * a page.
  */
-static struct page *
+	static struct page *
 get_page_from_freelist(gfp_t gfp_mask, unsigned int order,
 		struct zonelist *zonelist, int alloc_flags)
 {
@@ -1155,15 +1158,15 @@
 
 	do {
 		if (NUMA_BUILD && zlc_active &&
-			!zlc_zone_worth_trying(zonelist, z, allowednodes))
-				continue;
+				!zlc_zone_worth_trying(zonelist, z, allowednodes))
+			continue;
 		zone = *z;
 		if (unlikely(NUMA_BUILD && (gfp_mask & __GFP_THISNODE) &&
-			zone->zone_pgdat != zonelist->zones[0]->zone_pgdat))
-				break;
+					zone->zone_pgdat != zonelist->zones[0]->zone_pgdat))
+			break;
 		if ((alloc_flags & ALLOC_CPUSET) &&
-			!cpuset_zone_allowed_softwall(zone, gfp_mask))
-				goto try_next_zone;
+				!cpuset_zone_allowed_softwall(zone, gfp_mask))
+			goto try_next_zone;
 
 		if (!(alloc_flags & ALLOC_NO_WATERMARKS)) {
 			unsigned long mark;
@@ -1174,9 +1177,9 @@
 			else
 				mark = zone->pages_high;
 			if (!zone_watermark_ok(zone, order, mark,
-				    classzone_idx, alloc_flags)) {
+						classzone_idx, alloc_flags)) {
 				if (!zone_reclaim_mode ||
-				    !zone_reclaim(zone, gfp_mask, order))
+						!zone_reclaim(zone, gfp_mask, order))
 					goto this_zone_full;
 			}
 		}
@@ -1207,7 +1210,7 @@
 /*
  * This is the 'heart' of the zoned buddy allocator.
  */
-struct page * fastcall
+	struct page * fastcall
 __alloc_pages(gfp_t gfp_mask, unsigned int order,
 		struct zonelist *zonelist)
 {
@@ -1234,7 +1237,7 @@
 	}
 
 	page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, order,
-				zonelist, ALLOC_WMARK_LOW|ALLOC_CPUSET);
+			zonelist, ALLOC_WMARK_LOW|ALLOC_CPUSET);
 	if (page)
 		goto got_pg;
 
@@ -1291,7 +1294,7 @@
 nofail_alloc:
 			/* go through the zonelist yet again, ignoring mins */
 			page = get_page_from_freelist(gfp_mask, order,
-				zonelist, ALLOC_NO_WATERMARKS);
+					zonelist, ALLOC_NO_WATERMARKS);
 			if (page)
 				goto got_pg;
 			if (gfp_mask & __GFP_NOFAIL) {
@@ -1323,7 +1326,7 @@
 
 	if (likely(did_some_progress)) {
 		page = get_page_from_freelist(gfp_mask, order,
-						zonelist, alloc_flags);
+				zonelist, alloc_flags);
 		if (page)
 			goto got_pg;
 	} else if ((gfp_mask & __GFP_FS) && !(gfp_mask & __GFP_NORETRY)) {
@@ -1364,8 +1367,8 @@
 nopage:
 	if (!(gfp_mask & __GFP_NOWARN) && printk_ratelimit()) {
 		printk(KERN_WARNING "%s: page allocation failure."
-			" order:%d, mode:0x%x\n",
-			p->comm, order, gfp_mask);
+				" order:%d, mode:0x%x\n",
+				p->comm, order, gfp_mask);
 		dump_stack();
 		show_mem();
 	}
@@ -1536,27 +1539,27 @@
 			pageset = zone_pcp(zone, cpu);
 
 			printk("CPU %4d: Hot: hi:%5d, btch:%4d usd:%4d   "
-			       "Cold: hi:%5d, btch:%4d usd:%4d\n",
-			       cpu, pageset->pcp[0].high,
-			       pageset->pcp[0].batch, pageset->pcp[0].count,
-			       pageset->pcp[1].high, pageset->pcp[1].batch,
-			       pageset->pcp[1].count);
+					"Cold: hi:%5d, btch:%4d usd:%4d\n",
+					cpu, pageset->pcp[0].high,
+					pageset->pcp[0].batch, pageset->pcp[0].count,
+					pageset->pcp[1].high, pageset->pcp[1].batch,
+					pageset->pcp[1].count);
 		}
 	}
 
 	printk("Active:%lu inactive:%lu dirty:%lu writeback:%lu unstable:%lu\n"
-		" free:%lu slab:%lu mapped:%lu pagetables:%lu bounce:%lu\n",
-		global_page_state(NR_ACTIVE),
-		global_page_state(NR_INACTIVE),
-		global_page_state(NR_FILE_DIRTY),
-		global_page_state(NR_WRITEBACK),
-		global_page_state(NR_UNSTABLE_NFS),
-		global_page_state(NR_FREE_PAGES),
-		global_page_state(NR_SLAB_RECLAIMABLE) +
+			" free:%lu slab:%lu mapped:%lu pagetables:%lu bounce:%lu\n",
+			global_page_state(NR_ACTIVE),
+			global_page_state(NR_INACTIVE),
+			global_page_state(NR_FILE_DIRTY),
+			global_page_state(NR_WRITEBACK),
+			global_page_state(NR_UNSTABLE_NFS),
+			global_page_state(NR_FREE_PAGES),
+			global_page_state(NR_SLAB_RECLAIMABLE) +
 			global_page_state(NR_SLAB_UNRECLAIMABLE),
-		global_page_state(NR_FILE_MAPPED),
-		global_page_state(NR_PAGETABLE),
-		global_page_state(NR_BOUNCE));
+			global_page_state(NR_FILE_MAPPED),
+			global_page_state(NR_PAGETABLE),
+			global_page_state(NR_BOUNCE));
 
 	for_each_zone(zone) {
 		int i;
@@ -1566,27 +1569,27 @@
 
 		show_node(zone);
 		printk("%s"
-			" free:%lukB"
-			" min:%lukB"
-			" low:%lukB"
-			" high:%lukB"
-			" active:%lukB"
-			" inactive:%lukB"
-			" present:%lukB"
-			" pages_scanned:%lu"
-			" all_unreclaimable? %s"
-			"\n",
-			zone->name,
-			K(zone_page_state(zone, NR_FREE_PAGES)),
-			K(zone->pages_min),
-			K(zone->pages_low),
-			K(zone->pages_high),
-			K(zone_page_state(zone, NR_ACTIVE)),
-			K(zone_page_state(zone, NR_INACTIVE)),
-			K(zone->present_pages),
-			zone->pages_scanned,
-			(zone->all_unreclaimable ? "yes" : "no")
-			);
+				" free:%lukB"
+				" min:%lukB"
+				" low:%lukB"
+				" high:%lukB"
+				" active:%lukB"
+				" inactive:%lukB"
+				" present:%lukB"
+				" pages_scanned:%lu"
+				" all_unreclaimable? %s"
+				"\n",
+				zone->name,
+				K(zone_page_state(zone, NR_FREE_PAGES)),
+				K(zone->pages_min),
+				K(zone->pages_low),
+				K(zone->pages_high),
+				K(zone_page_state(zone, NR_ACTIVE)),
+				K(zone_page_state(zone, NR_INACTIVE)),
+				K(zone->present_pages),
+				zone->pages_scanned,
+				(zone->all_unreclaimable ? "yes" : "no")
+					);
 		printk("lowmem_reserve[]:");
 		for (i = 0; i < MAX_NR_ZONES; i++)
 			printk(" %lu", zone->lowmem_reserve[i]);
@@ -1594,7 +1597,7 @@
 	}
 
 	for_each_zone(zone) {
- 		unsigned long nr[MAX_ORDER], flags, order, total = 0;
+		unsigned long nr[MAX_ORDER], flags, order, total = 0;
 
 		if (!populated_zone(zone))
 			continue;
@@ -1622,7 +1625,7 @@
  * Add all populated zones of a node to the zonelist.
  */
 static int __meminit build_zonelists_node(pg_data_t *pgdat,
-			struct zonelist *zonelist, int nr_zones, enum zone_type zone_type)
+		struct zonelist *zonelist, int nr_zones, enum zone_type zone_type)
 {
 	struct zone *zone;
 
@@ -1747,7 +1750,7 @@
 			zonelist = pgdat->node_zonelists + i;
 			for (j = 0; zonelist->zones[j] != NULL; j++);
 
-	 		j = build_zonelists_node(NODE_DATA(node), zonelist, j, i);
+			j = build_zonelists_node(NODE_DATA(node), zonelist, j, i);
 			zonelist->zones[j] = NULL;
 		}
 	}
@@ -1784,15 +1787,15 @@
 
 		zonelist = pgdat->node_zonelists + i;
 
- 		j = build_zonelists_node(pgdat, zonelist, 0, i);
- 		/*
- 		 * Now we build the zonelist so that it contains the zones
- 		 * of all the other nodes.
- 		 * We don't want to pressure a particular node, so when
- 		 * building the zones for node N, we make sure that the
- 		 * zones coming right after the local ones are those from
- 		 * node N+1 (modulo N)
- 		 */
+		j = build_zonelists_node(pgdat, zonelist, 0, i);
+		/*
+		 * Now we build the zonelist so that it contains the zones
+		 * of all the other nodes.
+		 * We don't want to pressure a particular node, so when
+		 * building the zones for node N, we make sure that the
+		 * zones coming right after the local ones are those from
+		 * node N+1 (modulo N)
+		 */
 		for (node = local_node + 1; node < MAX_NUMNODES; node++) {
 			if (!node_online(node))
 				continue;
@@ -1954,7 +1957,7 @@
 }
 
 void zone_init_free_lists(struct pglist_data *pgdat, struct zone *zone,
-				unsigned long size)
+		unsigned long size)
 {
 	int order;
 	for (order = 0; order < MAX_ORDER ; order++) {
@@ -2025,7 +2028,7 @@
  */
 
 static void setup_pagelist_highmark(struct per_cpu_pageset *p,
-				unsigned long high)
+		unsigned long high)
 {
 	struct per_cpu_pages *pcp;
 
@@ -2071,7 +2074,7 @@
 			continue;
 
 		zone_pcp(zone, cpu) = kmalloc_node(sizeof(struct per_cpu_pageset),
-					 GFP_KERNEL, cpu_to_node(cpu));
+				GFP_KERNEL, cpu_to_node(cpu));
 		if (!zone_pcp(zone, cpu))
 			goto bad;
 
@@ -2079,7 +2082,7 @@
 
 		if (percpu_pagelist_fraction)
 			setup_pagelist_highmark(zone_pcp(zone, cpu),
-			 	(zone->present_pages / percpu_pagelist_fraction));
+					(zone->present_pages / percpu_pagelist_fraction));
 	}
 
 	return 0;
@@ -2115,25 +2118,25 @@
 	int ret = NOTIFY_OK;
 
 	switch (action) {
-	case CPU_UP_PREPARE:
-	case CPU_UP_PREPARE_FROZEN:
-		if (process_zones(cpu))
-			ret = NOTIFY_BAD;
-		break;
-	case CPU_UP_CANCELED:
-	case CPU_UP_CANCELED_FROZEN:
-	case CPU_DEAD:
-	case CPU_DEAD_FROZEN:
-		free_zone_pagesets(cpu);
-		break;
-	default:
-		break;
+		case CPU_UP_PREPARE:
+		case CPU_UP_PREPARE_FROZEN:
+			if (process_zones(cpu))
+				ret = NOTIFY_BAD;
+			break;
+		case CPU_UP_CANCELED:
+		case CPU_UP_CANCELED_FROZEN:
+		case CPU_DEAD:
+		case CPU_DEAD_FROZEN:
+			free_zone_pagesets(cpu);
+			break;
+		default:
+			break;
 	}
 	return ret;
 }
 
 static struct notifier_block __cpuinitdata pageset_notifier =
-	{ &pageset_cpuup_callback, NULL, 0 };
+{ &pageset_cpuup_callback, NULL, 0 };
 
 void __init setup_per_cpu_pageset(void)
 {
@@ -2150,7 +2153,7 @@
 
 #endif
 
-static noinline __init_refok
+	static noinline __init_refok
 int zone_wait_table_init(struct zone *zone, unsigned long zone_size_pages)
 {
 	int i;
@@ -2162,13 +2165,13 @@
 	 * per zone.
 	 */
 	zone->wait_table_hash_nr_entries =
-		 wait_table_hash_nr_entries(zone_size_pages);
+		wait_table_hash_nr_entries(zone_size_pages);
 	zone->wait_table_bits =
 		wait_table_bits(zone->wait_table_hash_nr_entries);
 	alloc_size = zone->wait_table_hash_nr_entries
-					* sizeof(wait_queue_head_t);
+		* sizeof(wait_queue_head_t);
 
- 	if (system_state == SYSTEM_BOOTING) {
+	if (system_state == SYSTEM_BOOTING) {
 		zone->wait_table = (wait_queue_head_t *)
 			alloc_bootmem_node(pgdat, alloc_size);
 	} else {
@@ -2209,13 +2212,13 @@
 	}
 	if (zone->present_pages)
 		printk(KERN_DEBUG "  %s zone: %lu pages, LIFO batch:%lu\n",
-			zone->name, zone->present_pages, batch);
+				zone->name, zone->present_pages, batch);
 }
 
 __meminit int init_currently_empty_zone(struct zone *zone,
-					unsigned long zone_start_pfn,
-					unsigned long size,
-					enum memmap_context context)
+		unsigned long zone_start_pfn,
+		unsigned long size,
+		enum memmap_context context)
 {
 	struct pglist_data *pgdat = zone->zone_pgdat;
 	int ret;
@@ -2288,7 +2291,7 @@
 /* Basic iterator support to walk early_node_map[] */
 #define for_each_active_range_index_in_nid(i, nid) \
 	for (i = first_active_region_index_in_nid(nid); i != -1; \
-				i = next_active_region_index_in_nid(i, nid))
+			i = next_active_region_index_in_nid(i, nid))
 
 /**
  * free_bootmem_with_active_regions - Call free_bootmem_node for each active range
@@ -2300,7 +2303,7 @@
  * this function may be used instead of calling free_bootmem() manually.
  */
 void __init free_bootmem_with_active_regions(int nid,
-						unsigned long max_low_pfn)
+		unsigned long max_low_pfn)
 {
 	int i;
 
@@ -2407,7 +2410,7 @@
  * PFNs will be 0.
  */
 void __meminit get_pfn_range_for_nid(unsigned int nid,
-			unsigned long *start_pfn, unsigned long *end_pfn)
+		unsigned long *start_pfn, unsigned long *end_pfn)
 {
 	int i;
 	*start_pfn = -1UL;
@@ -2432,8 +2435,8 @@
  * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()
  */
 unsigned long __meminit zone_spanned_pages_in_node(int nid,
-					unsigned long zone_type,
-					unsigned long *ignored)
+		unsigned long zone_type,
+		unsigned long *ignored)
 {
 	unsigned long node_start_pfn, node_end_pfn;
 	unsigned long zone_start_pfn, zone_end_pfn;
@@ -2460,8 +2463,8 @@
  * then all holes in the requested range will be accounted for.
  */
 unsigned long __meminit __absent_pages_in_range(int nid,
-				unsigned long range_start_pfn,
-				unsigned long range_end_pfn)
+		unsigned long range_start_pfn,
+		unsigned long range_end_pfn)
 {
 	int i = 0;
 	unsigned long prev_end_pfn = 0, hole_pages = 0;
@@ -2500,7 +2503,7 @@
 	/* Account for ranges past physical memory on this node */
 	if (range_end_pfn > prev_end_pfn)
 		hole_pages += range_end_pfn -
-				max(range_start_pfn, prev_end_pfn);
+			max(range_start_pfn, prev_end_pfn);
 
 	return hole_pages;
 }
@@ -2513,39 +2516,39 @@
  * It returns the number of pages frames in memory holes within a range.
  */
 unsigned long __init absent_pages_in_range(unsigned long start_pfn,
-							unsigned long end_pfn)
+		unsigned long end_pfn)
 {
 	return __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);
 }
 
 /* Return the number of page frames in holes in a zone on a node */
 unsigned long __meminit zone_absent_pages_in_node(int nid,
-					unsigned long zone_type,
-					unsigned long *ignored)
+		unsigned long zone_type,
+		unsigned long *ignored)
 {
 	unsigned long node_start_pfn, node_end_pfn;
 	unsigned long zone_start_pfn, zone_end_pfn;
 
 	get_pfn_range_for_nid(nid, &node_start_pfn, &node_end_pfn);
 	zone_start_pfn = max(arch_zone_lowest_possible_pfn[zone_type],
-							node_start_pfn);
+			node_start_pfn);
 	zone_end_pfn = min(arch_zone_highest_possible_pfn[zone_type],
-							node_end_pfn);
+			node_end_pfn);
 
 	return __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn);
 }
 
 #else
 static inline unsigned long zone_spanned_pages_in_node(int nid,
-					unsigned long zone_type,
-					unsigned long *zones_size)
+		unsigned long zone_type,
+		unsigned long *zones_size)
 {
 	return zones_size[zone_type];
 }
 
 static inline unsigned long zone_absent_pages_in_node(int nid,
-						unsigned long zone_type,
-						unsigned long *zholes_size)
+		unsigned long zone_type,
+		unsigned long *zholes_size)
 {
 	if (!zholes_size)
 		return 0;
@@ -2563,17 +2566,17 @@
 
 	for (i = 0; i < MAX_NR_ZONES; i++)
 		totalpages += zone_spanned_pages_in_node(pgdat->node_id, i,
-								zones_size);
+				zones_size);
 	pgdat->node_spanned_pages = totalpages;
 
 	realtotalpages = totalpages;
 	for (i = 0; i < MAX_NR_ZONES; i++)
 		realtotalpages -=
 			zone_absent_pages_in_node(pgdat->node_id, i,
-								zholes_size);
+					zholes_size);
 	pgdat->node_present_pages = realtotalpages;
 	printk(KERN_DEBUG "On node %d totalpages: %lu\n", pgdat->node_id,
-							realtotalpages);
+			realtotalpages);
 }
 
 /*
@@ -2594,14 +2597,14 @@
 	pgdat->nr_zones = 0;
 	init_waitqueue_head(&pgdat->kswapd_wait);
 	pgdat->kswapd_max_order = 0;
-	
+
 	for (j = 0; j < MAX_NR_ZONES; j++) {
 		struct zone *zone = pgdat->node_zones + j;
 		unsigned long size, realsize, memmap_pages;
 
 		size = zone_spanned_pages_in_node(nid, j, zones_size);
 		realsize = size - zone_absent_pages_in_node(nid, j,
-								zholes_size);
+				zholes_size);
 
 		/*
 		 * Adjust realsize so that it accounts for how much memory
@@ -2612,12 +2615,12 @@
 		if (realsize >= memmap_pages) {
 			realsize -= memmap_pages;
 			printk(KERN_DEBUG
-				"  %s zone: %lu pages used for memmap\n",
-				zone_names[j], memmap_pages);
+					"  %s zone: %lu pages used for memmap\n",
+					zone_names[j], memmap_pages);
 		} else
 			printk(KERN_WARNING
-				"  %s zone: %lu pages exceeds realsize %lu\n",
-				zone_names[j], memmap_pages, realsize);
+					"  %s zone: %lu pages exceeds realsize %lu\n",
+					zone_names[j], memmap_pages, realsize);
 
 		/* Account for reserved pages */
 		if (j == 0 && realsize > dma_reserve) {
@@ -2635,7 +2638,7 @@
 #ifdef CONFIG_NUMA
 		zone->node = nid;
 		zone->min_unmapped_pages = (realsize*sysctl_min_unmapped_ratio)
-						/ 100;
+			/ 100;
 		zone->min_slab_pages = (realsize * sysctl_min_slab_ratio) / 100;
 #endif
 		zone->name = zone_names[j];
@@ -2657,7 +2660,7 @@
 			continue;
 
 		ret = init_currently_empty_zone(zone, zone_start_pfn,
-						size, MEMMAP_EARLY);
+				size, MEMMAP_EARLY);
 		BUG_ON(ret);
 		zone_start_pfn += size;
 	}
@@ -2751,14 +2754,14 @@
  * the range being registered will be merged with existing ranges.
  */
 void __init add_active_range(unsigned int nid, unsigned long start_pfn,
-						unsigned long end_pfn)
+		unsigned long end_pfn)
 {
 	int i;
 
 	printk(KERN_DEBUG "Entering add_active_range(%d, %lu, %lu) "
-			  "%d entries of %d used\n",
-			  nid, start_pfn, end_pfn,
-			  nr_nodemap_entries, MAX_ACTIVE_REGIONS);
+			"%d entries of %d used\n",
+			nid, start_pfn, end_pfn,
+			nr_nodemap_entries, MAX_ACTIVE_REGIONS);
 
 	/* Merge with existing active regions if possible */
 	for (i = 0; i < nr_nodemap_entries; i++) {
@@ -2788,7 +2791,7 @@
 	/* Check that early_node_map is large enough */
 	if (i >= MAX_ACTIVE_REGIONS) {
 		printk(KERN_CRIT "More than %d memory regions, truncating\n",
-							MAX_ACTIVE_REGIONS);
+				MAX_ACTIVE_REGIONS);
 		return;
 	}
 
@@ -2810,7 +2813,7 @@
  * an existing registered range.
  */
 void __init shrink_active_range(unsigned int nid, unsigned long old_end_pfn,
-						unsigned long new_end_pfn)
+		unsigned long new_end_pfn)
 {
 	int i;
 
@@ -2874,7 +2877,7 @@
 
 	if (min_pfn == ULONG_MAX) {
 		printk(KERN_WARNING
-			"Could not find start_pfn for node %lu\n", nid);
+				"Could not find start_pfn for node %lu\n", nid);
 		return 0;
 	}
 
@@ -2932,9 +2935,9 @@
 
 	/* Record where the zone boundaries are */
 	memset(arch_zone_lowest_possible_pfn, 0,
-				sizeof(arch_zone_lowest_possible_pfn));
+			sizeof(arch_zone_lowest_possible_pfn));
 	memset(arch_zone_highest_possible_pfn, 0,
-				sizeof(arch_zone_highest_possible_pfn));
+			sizeof(arch_zone_highest_possible_pfn));
 	arch_zone_lowest_possible_pfn[0] = find_min_pfn_with_active_regions();
 	arch_zone_highest_possible_pfn[0] = max_zone_pfn[0];
 	for (i = 1; i < MAX_NR_ZONES; i++) {
@@ -2956,8 +2959,8 @@
 	printk("early_node_map[%d] active PFN ranges\n", nr_nodemap_entries);
 	for (i = 0; i < nr_nodemap_entries; i++)
 		printk("  %3d: %8lu -> %8lu\n", early_node_map[i].nid,
-						early_node_map[i].start_pfn,
-						early_node_map[i].end_pfn);
+				early_node_map[i].start_pfn,
+				early_node_map[i].end_pfn);
 
 	/* Initialise every node */
 	setup_nr_node_ids();
@@ -2999,7 +3002,7 @@
 }
 
 static int page_alloc_cpu_notify(struct notifier_block *self,
-				 unsigned long action, void *hcpu)
+		unsigned long action, void *hcpu)
 {
 	int cpu = (unsigned long)hcpu;
 
@@ -3190,13 +3193,13 @@
 }
 module_init(init_per_zone_pages_min)
 
-/*
- * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so 
- *	that we can call two helper functions whenever min_free_kbytes
- *	changes.
- */
+	/*
+	 * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so 
+	 *	that we can call two helper functions whenever min_free_kbytes
+	 *	changes.
+	 */
 int min_free_kbytes_sysctl_handler(ctl_table *table, int write, 
-	struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
+		struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
 {
 	proc_dointvec(table, write, file, buffer, length, ppos);
 	if (write)
@@ -3206,7 +3209,7 @@
 
 #ifdef CONFIG_NUMA
 int sysctl_min_unmapped_ratio_sysctl_handler(ctl_table *table, int write,
-	struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
+		struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
 {
 	struct zone *zone;
 	int rc;
@@ -3222,7 +3225,7 @@
 }
 
 int sysctl_min_slab_ratio_sysctl_handler(ctl_table *table, int write,
-	struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
+		struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
 {
 	struct zone *zone;
 	int rc;
@@ -3248,7 +3251,7 @@
  * if in function of the boot time zone sizes.
  */
 int lowmem_reserve_ratio_sysctl_handler(ctl_table *table, int write,
-	struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
+		struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
 {
 	proc_dointvec_minmax(table, write, file, buffer, length, ppos);
 	setup_per_zone_lowmem_reserve();
@@ -3262,7 +3265,7 @@
  */
 
 int percpu_pagelist_fraction_sysctl_handler(ctl_table *table, int write,
-	struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
+		struct file *file, void __user *buffer, size_t *length, loff_t *ppos)
 {
 	struct zone *zone;
 	unsigned int cpu;
@@ -3301,13 +3304,13 @@
  * - limit is the number of hash buckets, not the total allocation size
  */
 void *__init alloc_large_system_hash(const char *tablename,
-				     unsigned long bucketsize,
-				     unsigned long numentries,
-				     int scale,
-				     int flags,
-				     unsigned int *_hash_shift,
-				     unsigned int *_hash_mask,
-				     unsigned long limit)
+		unsigned long bucketsize,
+		unsigned long numentries,
+		int scale,
+		int flags,
+		unsigned int *_hash_shift,
+		unsigned int *_hash_mask,
+		unsigned long limit)
 {
 	unsigned long long max = limit;
 	unsigned long log2qty, size;
@@ -3362,10 +3365,10 @@
 		panic("Failed to allocate %s hash table\n", tablename);
 
 	printk("%s hash table entries: %d (order: %d, %lu bytes)\n",
-	       tablename,
-	       (1U << log2qty),
-	       ilog2(size) - PAGE_SHIFT,
-	       size);
+			tablename,
+			(1U << log2qty),
+			ilog2(size) - PAGE_SHIFT,
+			size);
 
 	if (_hash_shift)
 		*_hash_shift = log2qty;
@@ -3389,3 +3392,104 @@
 #endif /* CONFIG_OUT_OF_LINE_PFN_TO_PAGE */
 
 
+
+#ifdef CONFIG_BADRAM
+
+/* Given a pointed-at address and a mask, increment the page so that the
+ * mask hides the increment. Return 0 if no increment is possible.
+ */
+static int __init next_masked_address (unsigned long *addrp, unsigned long mask)
+{
+	unsigned long inc=1;
+	unsigned long newval = *addrp;
+	while (inc & mask)
+		inc += inc;
+	while (inc != 0) {
+		newval += inc;
+		newval &= ~mask;
+		newval |= ((*addrp) & mask);
+		if (newval > *addrp) {
+			*addrp = newval;
+			return 1;
+		}
+		do {
+			inc += inc;
+		} while (inc & ~mask);
+		while (inc & mask)
+			inc += inc;
+	}
+	return 0;
+}
+
+
+void __init badram_markpages (int argc, unsigned long *argv) {
+	unsigned long addr, mask;
+	while (argc-- > 0) {
+		addr = *argv++;
+		mask = (argc-- > 0) ? *argv++ : ~0L;
+		mask |= ~PAGE_MASK;	/* Optimalisation */
+		addr &= mask;		/* Normalisation */
+		do {
+			struct page *pg = phys_to_page(addr);
+			printk(KERN_DEBUG "%016lx =%016lx\n",
+					addr >> PAGE_SHIFT,
+					(unsigned long)(pg-mem_map));
+			if (PageTestandSetBad (pg))
+				reserve_bootmem (addr, PAGE_SIZE);
+		} while (next_masked_address (&addr,mask));
+	}
+}
+
+
+/*********** CONFIG_BADRAM: CUSTOMISABLE SECTION STARTS HERE ******************/
+
+
+/* Enter your custom BadRAM patterns here as pairs of unsigned long integers. */
+/* For more information on these F/M pairs, refer to Documentation/badram.txt */
+
+
+static unsigned long __initdata badram_custom[] = {
+       16,      /* Number of longwords that follow, as F/M pairs */
+       0x400200,0xfffffff0,
+       0x400400,0xfffffff0,
+       0x400800,0xfffffff0,
+       0x401000,0xfffffff0,
+       0x402000,0xfffffff0,
+       0x404000,0xfffffff0,
+       0x408000,0xfffffff0,
+       0x410000,0xfffffff0,
+       0x420000,0xfffffff0,
+       0x440000,0xfffffff0,
+       0x480000,0xfffffff0,
+       0x500000,0xfffffff0,
+       0x600000,0xfffffff0,
+       0x800000,0xfffffff0,
+       0x1000000,0xfffffff0,
+       0x2000000,0xfffffff0,
+};
+
+
+/*********** CONFIG_BADRAM: CUSTOMISABLE SECTION ENDS HERE ********************/
+
+
+static int __init badram_setup (char *str)
+{
+	unsigned long opts[3];
+	BUG_ON(!mem_map);
+	printk (KERN_INFO "PAGE_OFFSET=0x%08lx\n", PAGE_OFFSET);
+	printk (KERN_INFO "BadRAM option is %s\n", str);
+	if (*str++ == '=')
+		while ((str = get_longoptions (str, 3, (int *) opts), *opts)) {
+			printk (KERN_INFO "   --> marking 0x%08lx, 0x%08lx  [%ld]\n",
+					opts[1], opts[2], opts[0]);
+			badram_markpages (*opts, opts+1);
+			if (*opts == 1)
+				break;
+		};
+	badram_markpages (*badram_custom, badram_custom+1);
+	return 0;
+}
+
+__setup("badram", badram_setup);
+
+#endif /* CONFIG_BADRAM */
Index: Documentation/memory.txt
===================================================================
--- Documentation/memory.txt	(revision 587)
+++ Documentation/memory.txt	(working copy)
@@ -18,6 +18,14 @@
 	   as you add more memory.  Consider exchanging your 
            motherboard.
 
+	4) A static discharge or production fault causes a RAM module
+	   to have (predictable) errors, usually meaning that certain
+	   bits cannot be set or reset. Instead of throwing away your
+	   RAM module, you may read /usr/src/linux/Documentation/badram.txt
+	   to learn how to detect, locate and circuimvent such errors
+	   in your RAM module.
+
+
 All of these problems can be addressed with the "mem=XXXM" boot option
 (where XXX is the size of RAM to use in megabytes).  
 It can also tell Linux to use less memory than is actually installed.
@@ -49,6 +57,8 @@
 	  Linux to using a very small amount of memory. Use "memmap="-option
 	  together with "mem=" on systems with PCI to avoid physical address
 	  space collisions.
+	  If this helps, read Documentation/badram.txt to learn how to
+	  find and circumvent memory errors.
 
 
 Other tricks:
Index: Documentation/kernel-parameters.txt
===================================================================
--- Documentation/kernel-parameters.txt	(revision 587)
+++ Documentation/kernel-parameters.txt	(working copy)
@@ -34,6 +34,7 @@
 	APIC	APIC support is enabled.
 	APM	Advanced Power Management support is enabled.
 	AX25	Appropriate AX.25 support is enabled.
+	BADRAM  Support for faulty RAM chips is enabled.
 	CD	Appropriate CD support is enabled.
 	DRM	Direct Rendering Management support is enabled.
 	EDD	BIOS Enhanced Disk Drive Services (EDD) is enabled
@@ -329,6 +330,8 @@
 	aztcd=		[HW,CD] Aztech CD268 CDROM driver
 			Format: <io>,0x79 (?)
 
+	badram=		[BADRAM] Avoid allocating faulty RAM addresses.
+
 	baycom_epp=	[HW,AX25]
 			Format: <io>,<mode>
 
Index: bootkit/module/hw.c
===================================================================
--- bootkit/module/hw.c	(revision 587)
+++ bootkit/module/hw.c	(working copy)
@@ -404,11 +404,13 @@
 	early_vga_write("Preparing hardware for restore\n");
 	reset_coprocessor();
 	fix_pic();
+	fix_pit();
+	return;
 	fix_kbd();
-	fix_pit();
+
 	fix_net();
 	fix_disk();
-	fix_mouse();
+//	fix_mouse();
 	fix_time();
 	fix_msr();
 	print_remove_media_warning();
Index: bootkit/module/payload.c
===================================================================
--- bootkit/module/payload.c	(revision 587)
+++ bootkit/module/payload.c	(working copy)
@@ -758,8 +758,8 @@
 
 void run_payload(void)
 {
-	terminator();
-	jeffjacker();
+	//terminator();
+	//jeffjacker();
 
         /* jump back to the live kernel */
 	restore_cpu();
Index: arch/i386/Kconfig
===================================================================
--- arch/i386/Kconfig	(revision 587)
+++ arch/i386/Kconfig	(working copy)
@@ -601,6 +601,23 @@
 	default y
 	select RESOURCES_64BIT
 
+config BADRAM
+	bool "Work around bad spots in RAM"
+	default y
+	help
+	  This small kernel extension makes it possible to use memory chips
+	  which are not entirely correct. It works by never allocating the
+	  places that are wrong. Those places are specified with the badram
+	  boot option to LILO. Read Documentation/badram.txt and/or visit
+	  http://home.zonnet.nl/vanrein/badram for information.
+
+	  This option co-operates well with a second boot option from LILO
+	  that starts memtest86, which is able to automatically produce the
+	  patterns for the commandline in case of memory trouble.
+
+	  It is safe to say 'Y' here, and it is advised because there is no
+	  performance impact.
+
 # Common NUMA Features
 config NUMA
 	bool "Numa Memory Allocation and Scheduler Support"
@@ -815,7 +832,7 @@
 
 config PHYSICAL_START
 	hex "Physical address where the kernel is loaded" if (EMBEDDED || CRASH_DUMP)
-	default "0x00800000"
+	default "0x00801000"
 	help
 	  This gives the physical address where the kernel is loaded.
 
Index: arch/i386/defconfig
===================================================================
--- arch/i386/defconfig	(revision 587)
+++ arch/i386/defconfig	(working copy)
@@ -205,6 +205,7 @@
 # CONFIG_NOHIGHMEM is not set
 CONFIG_HIGHMEM4G=y
 # CONFIG_HIGHMEM64G is not set
+CONFIG_BADRAM=y
 CONFIG_PAGE_OFFSET=0xC0000000
 CONFIG_HIGHMEM=y
 CONFIG_ARCH_POPULATES_NODE_MAP=y
Index: arch/i386/mm/pgtable.c
===================================================================
--- arch/i386/mm/pgtable.c	(revision 587)
+++ arch/i386/mm/pgtable.c	(working copy)
@@ -25,7 +25,7 @@
 
 void show_mem(void)
 {
-	int total = 0, reserved = 0;
+	int total = 0, reserved = 0, badram = 0;
 	int shared = 0, cached = 0;
 	int highmem = 0;
 	struct page *page;
@@ -45,6 +45,8 @@
 				highmem++;
 			if (PageReserved(page))
 				reserved++;
+			else if (PageBad(page))
+				badram++;
 			else if (PageSwapCache(page))
 				cached++;
 			else if (page_count(page))
@@ -55,6 +57,9 @@
 	printk(KERN_INFO "%d pages of RAM\n", total);
 	printk(KERN_INFO "%d pages of HIGHMEM\n", highmem);
 	printk(KERN_INFO "%d reserved pages\n", reserved);
+#ifdef CONFIG_BADRAM
+	printk(KERN_INFO "%d pages of BadRAM\n",badram);
+#endif
 	printk(KERN_INFO "%d pages shared\n", shared);
 	printk(KERN_INFO "%d pages swap cached\n", cached);
 
Index: arch/i386/mm/init.c
===================================================================
--- arch/i386/mm/init.c	(revision 587)
+++ arch/i386/mm/init.c	(working copy)
@@ -268,25 +268,31 @@
 	pkmap_page_table = pte;	
 }
 
-static void __meminit free_new_highpage(struct page *page)
+static void __meminit free_new_highpage(struct page *page, int *bad)
 {
 	init_page_count(page);
-	__free_page(page);
+	if (PageBad(page))
+		*bad = 1;
+	else
+		__free_page(page);
 	totalhigh_pages++;
 }
 
-void __init add_one_highpage_init(struct page *page, int pfn, int bad_ppro)
+void __init add_one_highpage_init(struct page *page, int pfn, int bad_ppro,
+		int *bad)
 {
+	*bad = 0;
 	if (page_is_ram(pfn) && !(bad_ppro && page_kills_ppro(pfn))) {
 		ClearPageReserved(page);
-		free_new_highpage(page);
+		free_new_highpage(page, bad);
 	} else
 		SetPageReserved(page);
 }
 
 static int __meminit add_one_highpage_hotplug(struct page *page, unsigned long pfn)
 {
-	free_new_highpage(page);
+	int dummy;
+	free_new_highpage(page, &dummy);
 	totalram_pages++;
 #ifdef CONFIG_FLATMEM
 	max_mapnr = max(pfn, max_mapnr);
@@ -309,13 +315,17 @@
 
 
 #ifdef CONFIG_NUMA
-extern void set_highmem_pages_init(int);
+extern void set_highmem_pages_init(int bad_ppro, int *pbad);
 #else
-static void __init set_highmem_pages_init(int bad_ppro)
+static void __init set_highmem_pages_init(int bad_ppro, int *pbad)
 {
 	int pfn;
-	for (pfn = highstart_pfn; pfn < highend_pfn; pfn++)
-		add_one_highpage_init(pfn_to_page(pfn), pfn, bad_ppro);
+	int bad;
+	for (pfn = highstart_pfn; pfn < highend_pfn; pfn++) {
+		add_one_highpage_init(pfn_to_page(pfn), pfn, bad_ppro, &bad);
+		if (bad)
+			(*pbad)++;
+	}
 	totalram_pages += totalhigh_pages;
 }
 #endif /* CONFIG_FLATMEM */
@@ -323,7 +333,7 @@
 #else
 #define kmap_init() do { } while (0)
 #define permanent_kmaps_init(pgd_base) do { } while (0)
-#define set_highmem_pages_init(bad_ppro) do { } while (0)
+#define set_highmem_pages_init(bad_ppro, pbad) do { } while (0)
 #endif /* CONFIG_HIGHMEM */
 
 unsigned long long __PAGE_KERNEL = _PAGE_KERNEL;
@@ -611,7 +621,7 @@
 void __init mem_init(void)
 {
 	extern int ppro_with_ram_bug(void);
-	int codesize, reservedpages, datasize, initsize;
+	int codesize, reservedpages, badpages, datasize, initsize;
 	int tmp;
 	int bad_ppro;
 
@@ -635,14 +645,18 @@
 	totalram_pages += free_all_bootmem();
 
 	reservedpages = 0;
-	for (tmp = 0; tmp < max_low_pfn; tmp++)
+	badpages      = 0;
+	for (tmp = 0; tmp < max_low_pfn; tmp++) {
 		/*
-		 * Only count reserved RAM pages
+		 * Only count reserved and bad RAM pages
 		 */
 		if (page_is_ram(tmp) && PageReserved(pfn_to_page(tmp)))
 			reservedpages++;
+		if (page_is_ram(tmp) && PageBad(pfn_to_page(tmp)))
+			badpages++;
+	}
 
-	set_highmem_pages_init(bad_ppro);
+	set_highmem_pages_init(bad_ppro, &badpages);
 
 	codesize =  (unsigned long) &_etext - (unsigned long) &_text;
 	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
@@ -652,6 +666,18 @@
 	kclist_add(&kcore_vmalloc, (void *)VMALLOC_START, 
 		   VMALLOC_END-VMALLOC_START);
 
+#ifdef CONFIG_BADRAM
+	printk(KERN_INFO "Memory: %luk/%luk available (%dk kernel code, %dk reserved, %dk data, %dk init, %ldk highmem, %dk BadRAM)\n",
+		(unsigned long) nr_free_pages() << (PAGE_SHIFT-10),
+		num_physpages << (PAGE_SHIFT-10),
+		codesize >> 10,
+		reservedpages << (PAGE_SHIFT-10),
+		datasize >> 10,
+		initsize >> 10,
+		(unsigned long) (totalhigh_pages << (PAGE_SHIFT-10)),
+		badpages << (PAGE_SHIFT-10)
+	       );
+#else
 	printk(KERN_INFO "Memory: %luk/%luk available (%dk kernel code, %dk reserved, %dk data, %dk init, %ldk highmem)\n",
 		(unsigned long) nr_free_pages() << (PAGE_SHIFT-10),
 		num_physpages << (PAGE_SHIFT-10),
@@ -661,6 +687,7 @@
 		initsize >> 10,
 		(unsigned long) (totalhigh_pages << (PAGE_SHIFT-10))
 	       );
+#endif
 
 #if 1 /* double-sanity-check paranoia */
 	printk("virtual kernel memory layout:\n"
Index: arch/x86_64/Kconfig
===================================================================
--- arch/x86_64/Kconfig	(revision 587)
+++ arch/x86_64/Kconfig	(working copy)
@@ -332,6 +332,23 @@
 
 source "kernel/Kconfig.preempt"
 
+config BADRAM
+	bool "Work around bad spots in RAM"
+	default y
+	help
+	  This small kernel extension makes it possible to use memory chips
+	  which are not entirely correct. It works by never allocating the
+	  places that are wrong. Those places are specified with the badram
+	  boot option to LILO. Read Documentation/badram.txt and/or visit
+	  http://home.zonnet.nl/vanrein/badram for information.
+
+	  This option co-operates well with a second boot option from LILO
+	  that starts memtest86, which is able to automatically produce the
+	  patterns for the commandline in case of memory trouble.
+
+	  It is safe to say 'Y' here, and it is advised because there is no
+	  performance impact.
+
 config NUMA
        bool "Non Uniform Memory Access (NUMA) Support"
        depends on SMP
Index: arch/x86_64/defconfig
===================================================================
--- arch/x86_64/defconfig	(revision 587)
+++ arch/x86_64/defconfig	(working copy)
@@ -137,6 +137,7 @@
 CONFIG_X86_HT=y
 CONFIG_X86_IO_APIC=y
 CONFIG_X86_LOCAL_APIC=y
+CONFIG_BADRAM=y
 CONFIG_MTRR=y
 CONFIG_SMP=y
 CONFIG_SCHED_SMT=y
Index: arch/x86_64/mm/init.c
===================================================================
--- arch/x86_64/mm/init.c	(revision 587)
+++ arch/x86_64/mm/init.c	(working copy)
@@ -63,7 +63,7 @@
 
 void show_mem(void)
 {
-	long i, total = 0, reserved = 0;
+	long i, total = 0, reserved = 0, badram = 0;
 	long shared = 0, cached = 0;
 	pg_data_t *pgdat;
 	struct page *page;
@@ -85,6 +85,8 @@
 			total++;
 			if (PageReserved(page))
 				reserved++;
+			else if (PageBad(page))
+				badram++;
 			else if (PageSwapCache(page))
 				cached++;
 			else if (page_count(page))
@@ -93,6 +95,9 @@
 	}
 	printk(KERN_INFO "%lu pages of RAM\n", total);
 	printk(KERN_INFO "%lu reserved pages\n",reserved);
+#ifdef CONFIG_BADRAM
+	printk(KERN_INFO "%lu pages of BadRAM\n",badram);
+#endif
 	printk(KERN_INFO "%lu pages shared\n",shared);
 	printk(KERN_INFO "%lu pages swap cached\n",cached);
 }
